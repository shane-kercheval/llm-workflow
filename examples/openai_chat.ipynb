{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was original ran in a docker container where the project directory (i.e. same directory as README.md) is located in `/code`, which is set below. If you run locally you'll need to set the path of your project directory accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/code\n"
     ]
    }
   ],
   "source": [
    "%cd /code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `load_dotenv()` function below loads all the variables found in the `.env` file as environment variables. You must have a `.env` file located in the project directory containing your OpenAI API key, in the following format.\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=sk-...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import textwrap\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def wprint(string: str, max_width: int = 80) -> None:\n",
    "    \"\"\"Print `string` with a maximum widgth.\"\"\"\n",
    "    wrapped_string = textwrap.fill(string, max_width)\n",
    "    print(wrapped_string)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Chat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a simple example using `GPT-3.5` chat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The meaning of life is a philosophical question that has been debated for\n",
      "centuries. Different people and cultures have different beliefs and\n",
      "interpretations. Some believe that the meaning of life is to seek happiness and\n",
      "fulfillment, while others find meaning in religious or spiritual beliefs.\n",
      "Ultimately, the meaning of life may be subjective and can vary from person to\n",
      "person. It is up to each individual to explore and find their own sense of\n",
      "purpose and meaning in life.\n"
     ]
    }
   ],
   "source": [
    "from llm_workflow.openai import OpenAIChat\n",
    "\n",
    "model = OpenAIChat(model_name='gpt-3.5-turbo')\n",
    "response = model(\"What is the meaning of life?\")\n",
    "wprint(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can stream the response by providing a callback. The callback takes a single parameter of type `StreamingEvent` which has a `response` property containing each token streamed. In the example below, we simply print the response and end the printed message with the `|` character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The| meaning| of| life| is| a| philosophical| question| that| has| been| debated| for| centuries|.| Different| people| and| cultures| have| different| beliefs| and| interpretations|.| Some| believe| that| the| meaning| of| life| is| to| seek| happiness| and| fulfillment|,| while| others| find| meaning| in| religious| or| spiritual| beliefs|.| Ultimately|,| the| meaning| of| life| may| be| subjective| and| can| vary| from| person| to| person|.| It| is| up| to| each| individual| to| explore| and| find| their| own| sense| of| purpose| and| meaning| in| life|.|"
     ]
    }
   ],
   "source": [
    "from llm_workflow.openai import OpenAIChat\n",
    "\n",
    "model = OpenAIChat(\n",
    "    model_name='gpt-3.5-turbo',\n",
    "    streaming_callback=lambda x: print(x.response, end='|')\n",
    ")\n",
    "response = model(\"What is the meaning of life?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the previous example, the full text is returned at the end:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The meaning of life is a philosophical question that has been debated for\n",
      "centuries. Different people and cultures have different beliefs and\n",
      "interpretations. Some believe that the meaning of life is to seek happiness and\n",
      "fulfillment, while others find meaning in religious or spiritual beliefs.\n",
      "Ultimately, the meaning of life may be subjective and can vary from person to\n",
      "person. It is up to each individual to explore and find their own sense of\n",
      "purpose and meaning in life.\n"
     ]
    }
   ],
   "source": [
    "wprint(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage & Cost"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have access to the usage and costs through various properties below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of France is Paris.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llm_workflow.openai import OpenAIChat\n",
    "\n",
    "model = OpenAIChat(model_name='gpt-3.5-turbo')\n",
    "model(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Cost:            $0.00005\n",
      "Total Tokens:          31\n",
      "Total Prompt Tokens:   24\n",
      "Total Response Tokens: 7\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total Cost:            ${model.cost:.5f}\")\n",
    "print(f\"Total Tokens:          {model.total_tokens:,}\")\n",
    "print(f\"Total Prompt Tokens:   {model.input_tokens:,}\")\n",
    "print(f\"Total Response Tokens: {model.response_tokens:,}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use the same model/object again, the cost/usage will be incremented accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of Germany is Berlin.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(\"What is the capital of Germany?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Cost:            $0.00013\n",
      "Total Tokens:          84\n",
      "Total Prompt Tokens:   70\n",
      "Total Response Tokens: 14\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total Cost:            ${model.cost:.5f}\")\n",
    "print(f\"Total Tokens:          {model.total_tokens:,}\")\n",
    "print(f\"Total Prompt Tokens:   {model.input_tokens:,}\")\n",
    "print(f\"Total Response Tokens: {model.response_tokens:,}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## History"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `history()` method to get the prompt/response and cost/usage for each of the messages used by the model/object. \n",
    "\n",
    "There are two `ExchangeRecord` items in the list. The first item corresponds to the first question and the second item corresponds to the second question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ExchangeRecord(uuid='bc91d946-b89c-4a19-80df-ce1b978e5cd9', timestamp='2023-10-16 00:18:16.774', metadata={'model_name': 'gpt-3.5-turbo', 'temperature': 0, 'max_tokens': 2000, 'timeout': 10, 'messages': [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'What is the capital of France?'}]}, cost=5e-05, total_tokens=31, prompt='What is the capital of France?', response='The capital of France is Paris.', input_tokens=24, response_tokens=7),\n",
       " ExchangeRecord(uuid='47f9b0ed-93f0-49e1-8de9-afc2211f3e44', timestamp='2023-10-16 00:18:17.115', metadata={'model_name': 'gpt-3.5-turbo', 'temperature': 0, 'max_tokens': 2000, 'timeout': 10, 'messages': [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'What is the capital of France?'}, {'role': 'assistant', 'content': 'The capital of France is Paris.'}, {'role': 'user', 'content': 'What is the capital of Germany?'}]}, cost=8.3e-05, total_tokens=53, prompt='What is the capital of Germany?', response='The capital of Germany is Berlin.', input_tokens=46, response_tokens=7)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: What is the capital of France?\n",
      "Total Cost:            $0.00005\n",
      "Total Tokens:          31\n",
      "Total Prompt Tokens:   24\n",
      "Total Response Tokens: 7\n"
     ]
    }
   ],
   "source": [
    "print(f\"prompt: {model.history()[0].prompt}\")\n",
    "print(f\"Total Cost:            ${model.history()[0].cost:.5f}\")\n",
    "print(f\"Total Tokens:          {model.history()[0].total_tokens:,}\")\n",
    "print(f\"Total Prompt Tokens:   {model.history()[0].input_tokens:,}\")\n",
    "print(f\"Total Response Tokens: {model.history()[0].response_tokens:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: What is the capital of Germany?\n",
      "Total Cost:            $0.00008\n",
      "Total Tokens:          53\n",
      "Total Prompt Tokens:   46\n",
      "Total Response Tokens: 7\n"
     ]
    }
   ],
   "source": [
    "print(f\"prompt: {model.history()[1].prompt}\")\n",
    "print(f\"Total Cost:            ${model.history()[1].cost:.5f}\")\n",
    "print(f\"Total Tokens:          {model.history()[1].total_tokens:,}\")\n",
    "print(f\"Total Prompt Tokens:   {model.history()[1].input_tokens:,}\")\n",
    "print(f\"Total Response Tokens: {model.history()[1].response_tokens:,}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "31\n"
     ]
    }
   ],
   "source": [
    "print(len(\"What is the capital of France?\"))\n",
    "print(len(\"What is the capital of Germany?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Prompt Tokens (1st message):   24\n",
      "Total Prompt Tokens (2nd message):   46\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total Prompt Tokens (1st message):   {model.history()[0].input_tokens:,}\")\n",
    "print(f\"Total Prompt Tokens (2nd message):   {model.history()[1].input_tokens:,}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice above that even though the two questions we've ask are very close in size, the number of `input_tokens` has almost doubled in the second question/message. This is because we are sending the entire list of messages to the model for each new question so the model has the context of the entire conversation. (see the [memory.ipynb](https://github.com/shane-kercheval/llm-workflow/tree/main/examples/memory.ipynb) notebook for examples of limiting the memory).\n",
    "\n",
    "We can see exactly what we sent the model using the `_previous_messages` property below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system', 'content': 'You are a helpful assistant.'},\n",
       " {'role': 'user', 'content': 'What is the capital of France?'},\n",
       " {'role': 'assistant', 'content': 'The capital of France is Paris.'},\n",
       " {'role': 'user', 'content': 'What is the capital of Germany?'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._previous_messages"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
