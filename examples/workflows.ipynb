{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is ran in a docker container where the project directory (i.e. same directory as README.md) is located in `/code`, which is set below. If you run locally you'll need to set the path of your project directory accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/code\n"
     ]
    }
   ],
   "source": [
    "%cd /code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `load_dotenv` function below loads all the variables found in the `.env` file as environment variables. You must have a `.env` file located in the project directory containing your OpenAI API key, in the following format.\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=sk-...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from notebook_helpers import usage_string, mprint\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# workflows"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `workflow` is an object that executes a sequence of tasks. Each task in the workflow is a callable, which can be either a function or an object that implements the `__call__` method. **The output of one task serves as the input to the next task in the workflow. So a `workflow` is a simple mechanism that takes an input and sends the input to the first task, and the propegates the output of the first task to the second task, and so on, until the end of the workflow is reached, and returns the final result.** \n",
    "\n",
    "Furthermore, a workflow aggregates the history (prompts/responses, token usage, costs, etc.) across all tasks. More specifically, it aggregates all of the `Record` objects across any task that has a `history` method (which returns a list of Record objects; a Record object contains the metadata of an event such as costs, tokens used, prompt, response, etc.). This functionality allows the workflow to contain convenient methods that aggregate the costs and usage across all tasks of the workflow, and can also be used to explore intermediate steps and events in the workflow."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example - Using a model as a prompt enhancer\n",
    "\n",
    "Here's an example of a simple \"prompt enhancer\", where the user provides a prompt, and the first model enhances the user's prompt, then second model provides a response based on the enhanced prompt.\n",
    "\n",
    "- first task: defines a prompt-template that takes the user's prompt, and creates a new prompt asking a chat model to improve the prompt (within the context of creating python code)\n",
    "- second task: the `prompt_enhancer` model takes the modified prompt and improves the prompt\n",
    "- third task: the `chat_assistant` model takes the response from the last model (which is an improved prompt) and returns the request\n",
    "- fourth task: ignores the response from the chat model; creates a new prompt asking the chat model to extract the relevant code created in the previous response\n",
    "- fifth task: the chat model, which internally maintains the history of messages, returns only the relevant code from the previous response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "import re\n",
      "\n",
      "def mask_emails(text: str) -> str:\n",
      "    \"\"\"\n",
      "    Masks all email addresses in a given string.\n",
      "\n",
      "    Args:\n",
      "        text (str): The input string to be processed.\n",
      "\n",
      "    Returns:\n",
      "        str: The modified string with masked email addresses.\n",
      "\n",
      "    Examples:\n",
      "        >>> mask_emails(\"Contact us at john.doe@example.com for more information.\")\n",
      "        'Contact us at ***MASKED_EMAIL*** for more information.'\n",
      "\n",
      "        >>> mask_emails(\"Invalid email: john.doe@example\")\n",
      "        'Invalid email: john.doe@example'\n",
      "\n",
      "    Limitations:\n",
      "        - This function assumes that the input string contains valid email addresses.\n",
      "        - It may not handle all possible email address variations, as email address formats can be complex.\n",
      "    \"\"\"\n",
      "    email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
      "    masked_text = re.sub(email_pattern, '***MASKED_EMAIL***', text)\n",
      "    return masked_text\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "from llm_workflow.base import Workflow\n",
    "from llm_workflow.openai import OpenAIChat\n",
    "\n",
    "prompt_enhancer = OpenAIChat(model_name='gpt-3.5-turbo')\n",
    "# different model/object, therefore different message history (i.e. conversation)\n",
    "chat_assistant = OpenAIChat(model_name='gpt-3.5-turbo')\n",
    "\n",
    "def prompt_template(user_prompt: str) -> str:\n",
    "    return \"Improve the user's request, below, by expanding the request \" \\\n",
    "        \"to describe the relevant python best practices and documentation \" \\\n",
    "        f\"requirements that should be followed:\\n\\n```{user_prompt}```\"\n",
    "\n",
    "def prompt_extract_code(_) -> str:\n",
    "    # `_` signals that we are ignoring the input (from the previous task)\n",
    "    return \"Return only the primary code of interest from the previous answer, \"\\\n",
    "        \"including docstrings, but without any text/response.\"\n",
    "\n",
    "# The only requirement for the list of tasks is that each item/task is a\n",
    "# callable where the output of one task matches the input of the next task.\n",
    "# The input to the workflow is passed to the first task;\n",
    "# the output of the last task is returned by the workflow.\n",
    "workflow = Workflow(tasks=[\n",
    "    prompt_template,      # modifies the user's prompt\n",
    "    prompt_enhancer,      # returns an improved version of the user's prompt\n",
    "    chat_assistant,       # returns the chat response based on the improved prompt\n",
    "    prompt_extract_code,  # prompt to ask the model to extract only the relevant code\n",
    "    chat_assistant,       # returns only the function from the model's last response\n",
    "])\n",
    "prompt = \"create a function to mask all emails from a string value\"\n",
    "response = workflow(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost:            $0.0036\n",
      "Total Tokens:     2,098\n",
      "Prompt Tokens:    1,179\n",
      "Response Tokens:  919\n"
     ]
    }
   ],
   "source": [
    "print(f\"Cost:            ${workflow.sum('cost'):.4f}\")\n",
    "print(f\"Total Tokens:     {workflow.sum('total_tokens'):,}\")\n",
    "print(f\"Prompt Tokens:    {workflow.sum('prompt_tokens'):,}\")\n",
    "print(f\"Response Tokens:  {workflow.sum('response_tokens'):,}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view the history of the workflow (i.e. the aggregated history across all tasks) with the `workflow.history()` method. \n",
    "\n",
    "In this example, the only class that tracks history is `OpenAIChat`. Therefore, both the `prompt_enhancer` and `chat_assistant` objects will contain history. `workflow.history()` will return a list of three `ExchangeRecord` objects. The first record corresponds to our request to the `prompt_enhancer`, and the second two records correspond to our `chat_assistant` requests. An ExchangeRecord represents a single exchange/transaction with an LLM, encompassing an input (`prompt`) and its corresponding output (`response`), along with other properties like `cost` and `token_tokens`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[llm_workflow.models.ExchangeRecord,\n",
       " llm_workflow.models.ExchangeRecord,\n",
       " llm_workflow.models.ExchangeRecord]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[type(x) for x in workflow.history()]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view the response we received from the `prompt_enhancer` model by looking at the first record's `response` property (or the second record's `prompt` property since the workflow passes the output of `prompt_enhancer` as the input to the `chat_assistant`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I can help you create a function in Python that masks all emails from a given string value. To ensure best practices, it is recommended to follow the following steps:\n",
       "\n",
       "1. Start by defining a function with a descriptive name, such as `mask_emails`.\n",
       "2. Use regular expressions (regex) to identify email patterns within the string. The `re` module in Python provides useful functions for working with regex.\n",
       "3. Write a regex pattern to match email addresses. There are various patterns available, but a common one is `r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'`. This pattern matches most valid email addresses.\n",
       "4. Iterate through all matches found using the `re.findall()` function. This will return a list of email addresses.\n",
       "5. Replace each email address with a masked version. You can use a consistent mask, such as '***MASKED_EMAIL***', or modify the email address itself, such as replacing characters with asterisks.\n",
       "6. Return the modified string with masked email addresses.\n",
       "\n",
       "To ensure your code is well-documented, consider the following documentation requirements:\n",
       "\n",
       "1. Add a docstring at the beginning of the function to describe its purpose, input parameters, and return value.\n",
       "2. Include type hints for function parameters and return values to improve code readability.\n",
       "3. Provide examples of how to use the function in the docstring, demonstrating both valid and invalid inputs.\n",
       "4. If your function has any limitations or known issues, mention them in the documentation to set proper expectations for users.\n",
       "\n",
       "By following these best practices and documenting your code effectively, you can create a robust and user-friendly function to mask emails in a string value."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mprint(workflow.history()[0].response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also view the original response from the `chat_assistant` model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here's an example implementation of the `mask_emails` function in Python:\n",
       "\n",
       "```python\n",
       "import re\n",
       "\n",
       "def mask_emails(text: str) -> str:\n",
       "    \"\"\"\n",
       "    Masks all email addresses in a given string.\n",
       "\n",
       "    Args:\n",
       "        text (str): The input string to be processed.\n",
       "\n",
       "    Returns:\n",
       "        str: The modified string with masked email addresses.\n",
       "\n",
       "    Examples:\n",
       "        >>> mask_emails(\"Contact us at john.doe@example.com for more information.\")\n",
       "        'Contact us at ***MASKED_EMAIL*** for more information.'\n",
       "\n",
       "        >>> mask_emails(\"Invalid email: john.doe@example\")\n",
       "        'Invalid email: john.doe@example'\n",
       "\n",
       "    Limitations:\n",
       "        - This function assumes that the input string contains valid email addresses.\n",
       "        - It may not handle all possible email address variations, as email address formats can be complex.\n",
       "    \"\"\"\n",
       "    email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
       "    masked_text = re.sub(email_pattern, '***MASKED_EMAIL***', text)\n",
       "    return masked_text\n",
       "```\n",
       "\n",
       "In this example, the function takes a string `text` as input and returns a modified string with masked email addresses. It uses the `re.sub()` function to replace all email addresses found in the input string with the '***MASKED_EMAIL***' placeholder.\n",
       "\n",
       "The function includes a docstring that describes its purpose, input parameters, return value, and provides examples of how to use it. It also mentions the limitations of the function, such as assuming valid email addresses and potential variations in email address formats.\n",
       "\n",
       "Feel free to modify the function and its documentation to suit your specific needs."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mprint(workflow.history()[1].response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final response returned by the `chat_assistant` (and by the `workflow` object) returns only the `mask_email_addresses` function. The `response` object should match the `response` value in the last record (`workflow.history()[-1].response`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert response == workflow.history()[-1].response"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example -- Using the results of a web-search in a chat message."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following example, we will ask a chat model a question, and provide the model with additional context based on the most relevant text we find in a web-search. For the sake of the example, we will ask the chat model to the answer it provides and summarize that answer.  The workflow is defined as follows:\n",
    "\n",
    "\n",
    "|   input   |  output  |   task description   |\n",
    "|-----------|----------|----------|\n",
    "|  `None` |   str    |  Ask a question: e.g. `\"What is the meaning of life?\"`  |\n",
    "|    str    |   urls   |  Do a web-search via `DuckDuckGo` based on the question.  |\n",
    "|   urls    | Documents|  Take the urls, scrape the corresponding web-pages, and convert to a list of `Document` objects  |\n",
    "| Documents | Documents|  Split the `Document` objects into smaller chunks (also `Document` objects)  |\n",
    "| Documents | `None` |  Store the Documents in a `Chroma` document index so that we can lookup the most relevant documents |\n",
    "| `None`  |   str    |  Return the original question `What is the meaning of life?` |\n",
    "|    str    |   str    |  Take the original question, lookup the most relevant documents, and construct the corresponding prompt with the documents injected into the prompt. |\n",
    "|    str    |   str    |  Pass the prompt to the chat model; receive a response |\n",
    "|    str    |   str    |  Construct a new prompt, containing the response and a request to summarize the response. |\n",
    "|    str    |   str    |  Pass the new prompt to the chat model; receive a response |\n",
    "\n",
    "**Notice how the output of one task matches the input of the next task. This means you could swap out any of the objects we use below with your own custom functions or classes and the only requirement is that the input/output matches for that particular task.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial objects in workflow\n",
    "\n",
    "Let's define the objects and functions we are going to include in the workflow. We're including many tasks, so that we can see how flexible our workflow can be.\n",
    "\n",
    "Refer to the notebooks in https://github.com/shane-kercheval/llm-workflow/tree/main/examples for examples of how the various helper classes below (e.g. `DuckDuckGoSearch`, `scrape_url`, etc.) are used."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quick note on the `Value` object.\n",
    "\n",
    "The only bit of magic below is that we're using a `Value` object to cache the initial question, feed it into the web-search, and then inject it back into the workflow at a later point (passing it to the prompt-template). A `Value` object is a callable that, when called with a value caches and returns the same value, and when called without a value, simply returns the previously cached value. When you understand what it's doing, it's not magic at all. Here's a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a value\n",
      "This is a value\n",
      "new value\n",
      "new value\n"
     ]
    }
   ],
   "source": [
    "from llm_workflow.base import Value\n",
    "cache = Value()\n",
    "result = cache(\"This is a value\")  # calling the object with a value caches and returns the value\n",
    "print(result)  # the value in `result` is whatever was passed in to the Value object\n",
    "print(cache())  # calling the object without a parameter returns the cached value\n",
    "result = cache(\"new value\")\n",
    "print(result)\n",
    "print(cache())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from llm_workflow.base import Document, Workflow, Value\n",
    "from llm_workflow.openai import OpenAIEmbedding, OpenAIChat\n",
    "from llm_workflow.utilities import DuckDuckGoSearch, scrape_url, split_documents\n",
    "from llm_workflow.indexes import ChromaDocumentIndex\n",
    "from llm_workflow.prompt_templates import DocSearchTemplate\n",
    "\n",
    "# Seach DuckDuckGo based on initial question (passed into the workflow)\n",
    "duckduckgo_search = DuckDuckGoSearch(top_n=3)\n",
    "\n",
    "# define a function that takes the tasks from the web-search, scrapes the web-pages,\n",
    "# and then creates Document objects from the text of each web-page\n",
    "def scrape_urls(search_results):\n",
    "    \"\"\"\n",
    "    For each url (i.e. `href` in `search_results`):\n",
    "    - extracts text\n",
    "    - replace new-lines with spaces\n",
    "    - create a Document object\n",
    "    \"\"\"\n",
    "    return [\n",
    "        Document(content=re.sub(r'\\s+', ' ', scrape_url(x['href'])))\n",
    "        for x in search_results\n",
    "    ]\n",
    "\n",
    "# Embeddings model used for document index/search (the documents created from the web-search)\n",
    "embeddings_model = OpenAIEmbedding(model_name='text-embedding-ada-002')\n",
    "document_index = ChromaDocumentIndex(embeddings_model=embeddings_model, n_results=3)\n",
    "# DocSearchTemplate uses the ChromaDocumentIndex object to search for the most relevant documents\n",
    "# (from the web-search) based on the intitial question (which it takes as an input)\n",
    "prompt_template = DocSearchTemplate(doc_index=document_index, n_docs=3)\n",
    "# Create a chat model without streaming.\n",
    "non_streaming_chat = OpenAIChat(model_name='gpt-3.5-turbo')\n",
    "# Create a chat model with streaming enabled via callback.\n",
    "streaming_chat = OpenAIChat(\n",
    "    model_name='gpt-3.5-turbo',\n",
    "    streaming_callback=lambda x: print(x.response, end='|'),\n",
    ")\n",
    "\n",
    "# A `Value` object is a simple caching mechanism. It's a callable that, when passed a value, it\n",
    "# caches and returns that value; and when called without a value, it returns the cached value.\n",
    "# Below, it's being used to cache the original question, feed the question into the web-search\n",
    "# (`DuckDuckGoSearch`), and then re-inject the question back in the workflow and into the\n",
    "# prompt-template (`DocSearchTemplate`).\n",
    "question = Value()\n",
    "# This simple function takes the response from the original chat model and creates a prompt that\n",
    "# asks the model to summarize the response.\n",
    "question_2 = lambda x: f'Summarize the following in less than 20 words: \"{x}\"'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining and Running the workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lang|Chain| is| an| open|-source| framework| that| simpl|ifies| creating| applications| using| large| language| models|,| allowing| developers| to| combine| L|LM|s| with| external| computation| and| data|.|"
     ]
    }
   ],
   "source": [
    "# each task is a callable where the output of one task is the input to the next\n",
    "workflow = Workflow(tasks=[\n",
    "    question,\n",
    "    duckduckgo_search,\n",
    "    scrape_urls,\n",
    "    split_documents,  # split web-pages into smaller chunks; defaults to chunk-size of 500\n",
    "    document_index,  # __call__ function calls add() if given a list of documents (which is returned by `split_documents`)\n",
    "    question,\n",
    "    prompt_template,\n",
    "    non_streaming_chat,\n",
    "    question_2,\n",
    "    streaming_chat,\n",
    "])\n",
    "# the value passed into `workflow()` is passed to the `initial_question` object (which\n",
    "# is `Value` object and so it caches the value passed in and also returns it) which then gets\n",
    "# passed to the `duckduckgo_search` object, and so on.\n",
    "# the response of the final model is streamed, because our chat model has the streaming_callback\n",
    "# set, but it should also match the response returned\n",
    "response = workflow(\"What is langchain?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "> LangChain is an open-source framework that simplifies creating applications using large language models, allowing developers to combine LLMs with external computation and data."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mprint(\"> \" + response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total costs and usage\n",
    "\n",
    "The `workflow` object aggregates the costs/usage across all tasks that have a `history` property where that `history` property returns `UsageRecord` objects. In other words, any task that tracks its own history automatically gets counted towards the total usage within the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost:              $0.00158\n",
      "Total Tokens:       7,567\n",
      "Prompt Tokens:      446\n",
      "Response Tokens:    106\n",
      "Embedding Tokens:   7,015\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# usage_string simply uses the properties in workflow (workflow.sum('cost'), workflow.sum('total_tokens'), etc.)\n",
    "# to create a formatted string\n",
    "print(usage_string(workflow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7015"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to get the number of embedding tokens, sum `total_tokens` across only EmbeddingRecord objects\n",
    "from llm_workflow.base import EmbeddingRecord\n",
    "workflow.sum('total_tokens', types=EmbeddingRecord)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### History\n",
    "\n",
    "Similar to tracking the costs/usage, we can dig into the history at a more granular level.\n",
    "\n",
    "As you can see below, the `history` property returns a list containing:\n",
    "\n",
    "- a `SearchRecord` object capturing our original search query and results\n",
    "- two `EmbeddingsRecord` records; the first corresponds to getting the embeddings of all of the chunks from the web-pages; the second corresponds to getting the embedding of our original question so that we can find the most relevant chunks; the `EmbeddingsRecord` is a `UsageRecord` so it contains costs/usage\n",
    "- two `MessageRecord` records; the first corresponds to the first question (prompt & response) we made to the chat model; the second corresponds to our second query to the chat model asking it to summarize the first response; the `MessageRecord` is a `UsageRecord` so it contains costs/usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[llm_workflow.base.SearchRecord,\n",
       " llm_workflow.models.EmbeddingRecord,\n",
       " llm_workflow.models.EmbeddingRecord,\n",
       " llm_workflow.models.ExchangeRecord,\n",
       " llm_workflow.models.ExchangeRecord]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[type(x) for x in workflow.history()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp: 2023-10-01 02:29:52.505; prompt: \"Answer the question at the end of the text as trut...\"; response: \"LangChain is an open-source framework designed to ...\";  cost: $0.000662; total_tokens: 416; prompt_tokens: 340; response_tokens: 76; uuid: d52b6931-5749-49ad-9284-6c0c619a9b6e\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp: 2023-10-01 02:29:53.398; prompt: \"Summarize the following in less than 20 words: \"La...\"; response: \"LangChain is an open-source framework that simplif...\";  cost: $0.000219; total_tokens: 136; prompt_tokens: 106; response_tokens: 30; uuid: 7b0e6d3e-9733-47da-8a9c-ea509668643d\n"
     ]
    }
   ],
   "source": [
    "print(workflow.history()[3])\n",
    "mprint('---')\n",
    "print(workflow.history()[4])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `exchange_history`\n",
    "\n",
    "We can use the `exchange_history` property which simply returns all of the history items that are of type `ExchangeRecord`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llm_workflow.base import ExchangeRecord\n",
    "len(workflow.history(ExchangeRecord))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp: 2023-10-01 02:29:52.505; prompt: \"Answer the question at the end of the text as trut...\"; response: \"LangChain is an open-source framework designed to ...\";  cost: $0.000662; total_tokens: 416; prompt_tokens: 340; response_tokens: 76; uuid: d52b6931-5749-49ad-9284-6c0c619a9b6e\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp: 2023-10-01 02:29:53.398; prompt: \"Summarize the following in less than 20 words: \"La...\"; response: \"LangChain is an open-source framework that simplif...\";  cost: $0.000219; total_tokens: 136; prompt_tokens: 106; response_tokens: 30; uuid: 7b0e6d3e-9733-47da-8a9c-ea509668643d\n"
     ]
    }
   ],
   "source": [
    "print(workflow.history(ExchangeRecord)[0])  # same as workflow.history()[3]\n",
    "mprint('---')\n",
    "print(workflow.history(ExchangeRecord)[1])  # same as workflow.history()[4]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First prompt to the chat model\n",
    "\n",
    "You can see below that we used three chunks from our original web-search in the first prompt we sent to ChatGPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "Answer the question at the end of the text as truthfully and accurately as possible, based on the following information provided.\n",
       "\n",
       "Here is the information:\n",
       "\n",
       "```\n",
       "development framework LangChainDeveloper(s)Harrison ChaseInitial releaseOctober 2022Repositorygithub.com/hwchase17/langchainWritten inPython and JavaScriptTypeSoftware framework for large language model application developmentLicenseMIT LicenseWebsiteLangChain.com LangChain is a framework designed to simplify the creation of applications using large language models (LLMs). As a language model integration framework, LangChain's use-cases largely overlap with those of language models in general,\n",
       "\n",
       "including document analysis and summarization, chatbots, and code analysis.[1] Background[edit] LangChain was launched in October 2022 as an open source project by Harrison Chase, while working at machine learning startup Robust Intelligence. The project quickly garnered popularity, with improvements from hundreds of contributors on GitHub, trending discussions on Twitter, lively activity on the project's Discord server, many YouTube tutorials, and meetups in San Francisco and London. In April\n",
       "\n",
       "Article LikeLangChain is an open-source framework designed to simplify the creation of applications using large language models (LLMs). It provides a standard interface for chains, lots of integrations with other tools, and end-to-end chains for common applications. It allows AI developers to develop applications based on the combined Large Language Models (LLMs) such as GPT-4 with external sources of computation and data. This framework comes with a package for both Python and\n",
       "```\n",
       "\n",
       "Here is the question:\n",
       "\n",
       "```\n",
       "What is langchain?\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mprint(workflow.history(ExchangeRecord)[0].prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First response from the chat model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "> LangChain is an open-source framework designed to simplify the creation of applications using large language models (LLMs). It provides a standard interface for chains, integrations with other tools, and end-to-end chains for common applications. It allows AI developers to develop applications based on combined Large Language Models (LLMs) such as GPT-4 with external sources of computation and data."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mprint(\"> \" + workflow.history(ExchangeRecord)[0].response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second prompt to the chat model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Summarize the following in less than 20 words: \"LangChain is an open-source framework designed to simplify the creation of applications using large language models (LLMs). It provides a standard interface for chains, integrations with other tools, and end-to-end chains for common applications. It allows AI developers to develop applications based on combined Large Language Models (LLMs) such as GPT-4 with external sources of computation and data.\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mprint(workflow.history(ExchangeRecord)[1].prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second response from the chat model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "> LangChain is an open-source framework that simplifies creating applications using large language models, allowing developers to combine LLMs with external computation and data."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mprint(\"> \" + workflow.history(ExchangeRecord)[1].response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
