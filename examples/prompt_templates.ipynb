{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebook_helpers import mprint"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Templates\n",
    "\n",
    "A prompt-template is just a way to build different prompts to send to a chat model, based on pre-defined use-cases."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DocSearchTemplate\n",
    "\n",
    "A document search template is an object that searches a `DocumentIndex` based on a query, and inserts `n_docs` documents into the prompt, along with additional wording to the model asking it to use the information provided from the docs to answer the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_workflow.indexes import ChromaDocumentIndex\n",
    "from llm_workflow.prompt_templates import DocSearchTemplate\n",
    "\n",
    "doc_index = ChromaDocumentIndex()\n",
    "prompt_template = DocSearchTemplate(doc_index=doc_index, n_docs=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the default prompt-template used by `DocSearchTemplate`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "Answer the question at the end of the text as truthfully and accurately as possible, based on the following information provided.\n",
       "\n",
       "Here is the information:\n",
       "\n",
       "```\n",
       "{{documents}}\n",
       "```\n",
       "\n",
       "Here is the question:\n",
       "\n",
       "```\n",
       "{{prompt}}\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mprint(prompt_template.template)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add documents to our document index.\n",
    "\n",
    "**If we pass a list of documents to `doc_index`, the `__call__` method will pass the list to the `add()` method. If we pass a string or Document to `dock_index`, the `__call__` method will pass the value to the `search()` method.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_workflow.base import Document\n",
    "\n",
    "docs = [\n",
    "    Document(\n",
    "        content=\"The greatest basketball player of all time is Michael Jordan\",\n",
    "        metadata={'id': 1}\n",
    "    ),\n",
    "    Document(\n",
    "        content=\"The greatest three point shooter of all time is Steph Curry.\",\n",
    "        metadata={'id': 0}\n",
    "    ),\n",
    "    Document(\n",
    "        content=\"The greatest hockey player of all time is Wayne Gretzky.\",\n",
    "        metadata={'id': 2}\n",
    "    ),\n",
    "]\n",
    "# passing list[Document] is equivalent of calling `doc_index.add(docs)`\n",
    "doc_index(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(content='The greatest three point shooter of all time is Steph Curry.', metadata={'id': 0, 'distance': 0.35710838437080383})]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# passing a string (or Document) is equivalent of calling `doc_index.search(value)`\n",
    "doc_index(\"Who is the greatest 3-point shooter of all time?\", n_results=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's construct our prompt. The `DocSearchTemplate` object will retrieve the most relevant document (from the `ChromaDocumentIndex` object) based on the value we send it, and then inject that document into the prompt. Because we set `n_docs=1` above, it will only include one Document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "Answer the question at the end of the text as truthfully and accurately as possible, based on the following information provided.\n",
       "\n",
       "Here is the information:\n",
       "\n",
       "```\n",
       "The greatest three point shooter of all time is Steph Curry.\n",
       "```\n",
       "\n",
       "Here is the question:\n",
       "\n",
       "```\n",
       "Who is the greatest 3-point shooter of all time?\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = prompt_template(\"Who is the greatest 3-point shooter of all time?\")\n",
    "mprint(prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PythonObjectMetadataTemplate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To create a graph using the `my_credit_df` DataFrame and Plotly Express, you can use the following code:\n",
       "\n",
       "```python\n",
       "import plotly.express as px\n",
       "\n",
       "fig = px.scatter(my_credit_df, x='checking_balance', y='months_loan_duration')\n",
       "fig.show()\n",
       "```\n",
       "\n",
       "This code will create a scatter plot with the checking account balance on the x-axis and the duration of the loan on the y-axis."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llm_workflow.base import Workflow\n",
    "from llm_workflow.openai import OpenAIChat\n",
    "from llm_workflow.prompt_templates import PythonObjectMetadataTemplate\n",
    "import pandas as pd\n",
    "from notebook_helpers import mprint\n",
    "\n",
    "credit = pd.read_csv('/code/tests/test_data/data/credit.csv')\n",
    "\n",
    "prompt_template = PythonObjectMetadataTemplate(objects={'my_credit_df': credit})\n",
    "model = OpenAIChat()\n",
    "workflow = Workflow(tasks=[prompt_template, model])\n",
    "\n",
    "prompt = \"Create a graph using @my_credit_df and plotly express of checking account balance and \" \\\n",
    "    \"the duration of the loan.?\"\n",
    "response = workflow(prompt)\n",
    "mprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables Used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'my_credit_df'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template._extracted_variables_last_call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Template Used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "Answer the question at the end of the text as truthfully and accurately as possible. Use the metadata of the python objects as appropriate. Tailor your response according to the most relevant objects. Don't use the if they don't appear relevant.\n",
       "\n",
       "Here is the metadata:\n",
       "\n",
       "```\n",
       "A pd.DataFrame `my_credit_df` that contains the following numeric and non-numeric columns:\n",
       "\n",
       "\n",
       "Here are the numeric columns and corresponding summary statistics:\n",
       "\n",
       "                       count      mean          std    min     25%     50%  \\\n",
       "months_loan_duration  1000.0    20.903    12.058814    4.0    12.0    18.0   \n",
       "amount                1000.0  3271.258  2822.736876  250.0  1365.5  2319.5   \n",
       "percent_of_income     1000.0     2.973     1.118715    1.0     2.0     3.0   \n",
       "years_at_residence    1000.0     2.845     1.103718    1.0     2.0     3.0   \n",
       "age                   1000.0    35.546    11.375469   19.0    27.0    33.0   \n",
       "existing_loans_count  1000.0     1.407     0.577654    1.0     1.0     1.0   \n",
       "dependents            1000.0     1.155     0.362086    1.0     1.0     1.0   \n",
       "\n",
       "                          75%      max  \n",
       "months_loan_duration    24.00     72.0  \n",
       "amount                3972.25  18424.0  \n",
       "percent_of_income        4.00      4.0  \n",
       "years_at_residence       4.00      4.0  \n",
       "age                     42.00     75.0  \n",
       "existing_loans_count     2.00      4.0  \n",
       "dependents               1.00      2.0  \n",
       "\n",
       "Here are the non-numeric columns and corresponding value counts:\n",
       "\n",
       "`checking_balance`: {'unknown': 394, '< 0 DM': 274, '1 - 200 DM': 269, '> 200 DM': 63}\n",
       "`credit_history`: {'good': 530, 'critical': 293, 'poor': 88, 'very good': 49, 'perfect': 40}\n",
       "`purpose`: {'furniture/appliances': 473, 'car': 337, 'business': 97, 'education': 59, 'renovations': 22, 'car0': 12}\n",
       "`savings_balance`: {'< 100 DM': 603, 'unknown': 183, '100 - 500 DM': 103, '500 - 1000 DM': 63, '> 1000 DM': 48}\n",
       "`employment_duration`: {'1 - 4 years': 339, '> 7 years': 253, '4 - 7 years': 174, '< 1 year': 172, 'unemployed': 62}\n",
       "`other_credit`: {'none': 814, 'bank': 139, 'store': 47}\n",
       "`housing`: {'own': 713, 'rent': 179, 'other': 108}\n",
       "`job`: {'skilled': 630, 'unskilled': 200, 'management': 148, 'unemployed': 22}\n",
       "`phone`: {'no': 596, 'yes': 404}\n",
       "`default`: {'no': 700, 'yes': 300}\n",
       "\n",
       "\n",
       "Use both the numeric and non-numeric columns as appropriate.\n",
       "```\n",
       "\n",
       "----\n",
       "\n",
       "Here is the question:\n",
       "\n",
       "```\n",
       "Create a graph using @my_credit_df and plotly express of checking account balance and the duration of the loan.?\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mprint(workflow.history()[0].prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Cost:           $0.00103\n",
      "Total Tokens:          942\n",
      "Total Prompt Tokens:   856\n",
      "Total Response Tokens: 86\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total Cost:           ${model.cost:.5f}\")\n",
    "print(f\"Total Tokens:          {model.total_tokens:,}\")\n",
    "print(f\"Total Prompt Tokens:   {model.input_tokens:,}\")\n",
    "print(f\"Total Response Tokens: {model.response_tokens:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To determine the best predictors of `default` using the `my_df` DataFrame, we can use the logistic regression model from the statsmodels library in Python. We will include both numeric and non-numeric columns in the model.\n",
       "\n",
       "First, we need to process the non-numeric columns appropriately by encoding them into dummy variables. Then, we can create the logistic regression model and print the summary of all the coefficients.\n",
       "\n",
       "Here's a sample code to achieve this:\n",
       "\n",
       "```python\n",
       "import pandas as pd\n",
       "import statsmodels.api as sm\n",
       "\n",
       "# Process non-numeric columns by encoding them into dummy variables\n",
       "my_df_processed = pd.get_dummies(my_df, columns=['checking_balance', 'credit_history', 'purpose', 'savings_balance', 'employment_duration', 'other_credit', 'housing', 'job', 'phone'])\n",
       "\n",
       "# Define the independent variables (predictors) and the dependent variable\n",
       "X = my_df_processed.drop('default', axis=1)\n",
       "y = my_df['default']\n",
       "\n",
       "# Add a constant to the independent variables\n",
       "X = sm.add_constant(X)\n",
       "\n",
       "# Create the logistic regression model\n",
       "logit_model = sm.Logit(y, X)\n",
       "\n",
       "# Fit the model\n",
       "logit_result = logit_model.fit()\n",
       "\n",
       "# Print the summary of all the coefficients\n",
       "print(logit_result.summary())\n",
       "```\n",
       "\n",
       "This code processes the non-numeric columns by encoding them into dummy variables, creates the logistic regression model, fits the model, and then prints the summary of all the coefficients.\n",
       "\n",
       "The best predictors of `default` can be determined by examining the coefficients in the summary output. The columns with higher absolute coefficient values are likely to be the best predictors of `default`."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llm_workflow.base import Workflow\n",
    "from llm_workflow.openai import OpenAIChat\n",
    "from llm_workflow.prompt_templates import PythonObjectMetadataTemplate\n",
    "import pandas as pd\n",
    "from notebook_helpers import mprint\n",
    "\n",
    "credit = pd.read_csv('/code/tests/test_data/data/credit.csv')\n",
    "\n",
    "prompt_template = PythonObjectMetadataTemplate()\n",
    "model = OpenAIChat()\n",
    "workflow = Workflow(tasks=[prompt_template, model])\n",
    "\n",
    "# we can add objects in the constructor or dynamically\n",
    "prompt_template.add_object('my_df', credit)\n",
    "\n",
    "prompt = \"Using @my_df, which columns are going to be the best predictors of `default`? \" \\\n",
    "    \"Use these columns to create a logistic regression model using statsmodels in python. \" \\\n",
    "    \"Process numeric and non-numeric columns appropriately. \" \\\n",
    "    \"Print the summary of all of the coefficients.\"\n",
    "\n",
    "response = workflow(prompt)\n",
    "mprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables Used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'my_df'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template._extracted_variables_last_call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Template Used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "Answer the question at the end of the text as truthfully and accurately as possible. Use the metadata of the python objects as appropriate. Tailor your response according to the most relevant objects. Don't use the if they don't appear relevant.\n",
       "\n",
       "Here is the metadata:\n",
       "\n",
       "```\n",
       "A pd.DataFrame `my_df` that contains the following numeric and non-numeric columns:\n",
       "\n",
       "\n",
       "Here are the numeric columns and corresponding summary statistics:\n",
       "\n",
       "                       count      mean          std    min     25%     50%  \\\n",
       "months_loan_duration  1000.0    20.903    12.058814    4.0    12.0    18.0   \n",
       "amount                1000.0  3271.258  2822.736876  250.0  1365.5  2319.5   \n",
       "percent_of_income     1000.0     2.973     1.118715    1.0     2.0     3.0   \n",
       "years_at_residence    1000.0     2.845     1.103718    1.0     2.0     3.0   \n",
       "age                   1000.0    35.546    11.375469   19.0    27.0    33.0   \n",
       "existing_loans_count  1000.0     1.407     0.577654    1.0     1.0     1.0   \n",
       "dependents            1000.0     1.155     0.362086    1.0     1.0     1.0   \n",
       "\n",
       "                          75%      max  \n",
       "months_loan_duration    24.00     72.0  \n",
       "amount                3972.25  18424.0  \n",
       "percent_of_income        4.00      4.0  \n",
       "years_at_residence       4.00      4.0  \n",
       "age                     42.00     75.0  \n",
       "existing_loans_count     2.00      4.0  \n",
       "dependents               1.00      2.0  \n",
       "\n",
       "Here are the non-numeric columns and corresponding value counts:\n",
       "\n",
       "`checking_balance`: {'unknown': 394, '< 0 DM': 274, '1 - 200 DM': 269, '> 200 DM': 63}\n",
       "`credit_history`: {'good': 530, 'critical': 293, 'poor': 88, 'very good': 49, 'perfect': 40}\n",
       "`purpose`: {'furniture/appliances': 473, 'car': 337, 'business': 97, 'education': 59, 'renovations': 22, 'car0': 12}\n",
       "`savings_balance`: {'< 100 DM': 603, 'unknown': 183, '100 - 500 DM': 103, '500 - 1000 DM': 63, '> 1000 DM': 48}\n",
       "`employment_duration`: {'1 - 4 years': 339, '> 7 years': 253, '4 - 7 years': 174, '< 1 year': 172, 'unemployed': 62}\n",
       "`other_credit`: {'none': 814, 'bank': 139, 'store': 47}\n",
       "`housing`: {'own': 713, 'rent': 179, 'other': 108}\n",
       "`job`: {'skilled': 630, 'unskilled': 200, 'management': 148, 'unemployed': 22}\n",
       "`phone`: {'no': 596, 'yes': 404}\n",
       "`default`: {'no': 700, 'yes': 300}\n",
       "\n",
       "\n",
       "Use both the numeric and non-numeric columns as appropriate.\n",
       "```\n",
       "\n",
       "----\n",
       "\n",
       "Here is the question:\n",
       "\n",
       "```\n",
       "Using @my_df, which columns are going to be the best predictors of `default`? Use these columns to create a logistic regression model using statsmodels in python. Process numeric and non-numeric columns appropriately. Print the summary of all of the coefficients.\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mprint(workflow.history()[0].prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Cost:           $0.00155\n",
      "Total Tokens:          1,219\n",
      "Total Prompt Tokens:   883\n",
      "Total Response Tokens: 336\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total Cost:           ${model.cost:.5f}\")\n",
    "print(f\"Total Tokens:          {model.total_tokens:,}\")\n",
    "print(f\"Total Prompt Tokens:   {model.input_tokens:,}\")\n",
    "print(f\"Total Response Tokens: {model.response_tokens:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You're welcome! If you have any more questions or need further assistance, feel free to ask. Good luck with your logistic regression model!\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow(\"thank you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'thank you'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# no metadata used even though we are using same prompt\n",
    "workflow.history()[-1].prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Cost:           $0.00284\n",
      "Total Tokens:          2,476\n",
      "Total Prompt Tokens:   2,112\n",
      "Total Response Tokens: 364\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total Cost:           ${model.cost:.5f}\")\n",
    "print(f\"Total Tokens:          {model.total_tokens:,}\")\n",
    "print(f\"Total Prompt Tokens:   {model.input_tokens:,}\")\n",
    "print(f\"Total Response Tokens: {model.response_tokens:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
