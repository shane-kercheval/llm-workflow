{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebook_helpers import mprint"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Templates\n",
    "\n",
    "A prompt-template is just a way to build different prompts to send to a chat model, based on pre-defined use-cases."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DocSearchTemplate\n",
    "\n",
    "A document search template is an object that searches a `DocumentIndex` based on a query, and inserts `n_docs` documents into the prompt, along with additional wording to the model asking it to use the information provided from the docs to answer the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_workflow.indexes import ChromaDocumentIndex\n",
    "from llm_workflow.prompt_templates import DocSearchTemplate\n",
    "\n",
    "doc_index = ChromaDocumentIndex()\n",
    "prompt_template = DocSearchTemplate(doc_index=doc_index, n_docs=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the default prompt-template used by `DocSearchTemplate`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "Answer the question at the end of the text as truthfully and accurately as possible, based on the following information provided.\n",
       "\n",
       "Here is the information:\n",
       "\n",
       "```\n",
       "{{documents}}\n",
       "```\n",
       "\n",
       "Here is the question:\n",
       "\n",
       "```\n",
       "{{prompt}}\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mprint(prompt_template.template)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add documents to our document index.\n",
    "\n",
    "**If we pass a list of documents to `doc_index`, the `__call__` method will pass the list to the `add()` method. If we pass a string or Document to `dock_index`, the `__call__` method will pass the value to the `search()` method.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_workflow.base import Document\n",
    "\n",
    "docs = [\n",
    "    Document(\n",
    "        content=\"The greatest basketball player of all time is Michael Jordan\",\n",
    "        metadata={'id': 1},\n",
    "    ),\n",
    "    Document(\n",
    "        content=\"The greatest three point shooter of all time is Steph Curry.\",\n",
    "        metadata={'id': 0},\n",
    "    ),\n",
    "    Document(\n",
    "        content=\"The greatest hockey player of all time is Wayne Gretzky.\",\n",
    "        metadata={'id': 2},\n",
    "    ),\n",
    "]\n",
    "# passing list[Document] is equivalent of calling `doc_index.add(docs)`\n",
    "doc_index(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(content='The greatest three point shooter of all time is Steph Curry.', metadata={'id': 0, 'distance': 0.35710838437080383})]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# passing a string (or Document) is equivalent of calling `doc_index.search(value)`\n",
    "doc_index(\"Who is the greatest 3-point shooter of all time?\", n_results=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's construct our prompt. The `DocSearchTemplate` object will retrieve the most relevant document (from the `ChromaDocumentIndex` object) based on the value we send it, and then inject that document into the prompt. Because we set `n_docs=1` above, it will only include one Document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "Answer the question at the end of the text as truthfully and accurately as possible, based on the following information provided.\n",
       "\n",
       "Here is the information:\n",
       "\n",
       "```\n",
       "The greatest three point shooter of all time is Steph Curry.\n",
       "```\n",
       "\n",
       "Here is the question:\n",
       "\n",
       "```\n",
       "Who is the greatest 3-point shooter of all time?\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = prompt_template(\"Who is the greatest 3-point shooter of all time?\")\n",
    "mprint(prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PythonObjectMetadataTemplate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To group by the checking account and calculate the average duration of the loan using the `my_df` DataFrame, you can use the following code:\n",
       "\n",
       "```python\n",
       "average_loan_duration_by_checking = my_df.groupby('checking_balance')['months_loan_duration'].mean()\n",
       "print(average_loan_duration_by_checking)\n",
       "```\n",
       "\n",
       "This code will group the DataFrame by the checking account and calculate the average duration of the loan for each group, then print the resulting DataFrame."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llm_workflow.base import Workflow\n",
    "from llm_workflow.openai import OpenAIChat\n",
    "from llm_workflow.prompt_templates import PythonObjectMetadataTemplate\n",
    "import pandas as pd\n",
    "from notebook_helpers import mprint\n",
    "\n",
    "my_df = pd.read_csv('/code/tests/test_data/data/credit.csv')\n",
    "\n",
    "prompt_template = PythonObjectMetadataTemplate(objects={'my_df': my_df})\n",
    "model = OpenAIChat()\n",
    "workflow = Workflow(tasks=[prompt_template, model])\n",
    "\n",
    "# the prompt refers to the column 'checking_balance' as 'checking account'\n",
    "# but the model infers the correct column\n",
    "# same with 'duration' ('months_loan_duration')\n",
    "prompt = \"Using @my_df, group by checking account and calculate the average duration of the loan. \" \\\n",
    "    \"Print the resulting dataframe.\"\n",
    "\n",
    "response = workflow(prompt)\n",
    "mprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute Generated Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checking_balance\n",
      "1 - 200 DM    22.680297\n",
      "< 0 DM        21.339416\n",
      "> 200 DM      17.349206\n",
      "unknown       19.954315\n",
      "Name: months_loan_duration, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from llm_workflow.internal_utilities import extract_code_blocks\n",
    "\n",
    "exec(extract_code_blocks(response)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables Used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'my_df'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template._extracted_variables_last_call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Template Used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "Answer the question at the end of the text as truthfully and accurately as possible. Use the metadata of the python objects as appropriate. Tailor your response according to the most relevant objects. Don't use the metadata if they don't appear relevant.\n",
       "\n",
       "Here is the metadata:\n",
       "\n",
       "```\n",
       "A pd.DataFrame `my_df` that contains the following numeric and non-numeric columns:\n",
       "\n",
       "\n",
       "Here are the numeric columns and corresponding summary statistics:\n",
       "\n",
       "                       count      mean          std    min     25%     50%  \\\n",
       "months_loan_duration  1000.0    20.903    12.058814    4.0    12.0    18.0   \n",
       "amount                1000.0  3271.258  2822.736876  250.0  1365.5  2319.5   \n",
       "percent_of_income     1000.0     2.973     1.118715    1.0     2.0     3.0   \n",
       "years_at_residence    1000.0     2.845     1.103718    1.0     2.0     3.0   \n",
       "age                   1000.0    35.546    11.375469   19.0    27.0    33.0   \n",
       "existing_loans_count  1000.0     1.407     0.577654    1.0     1.0     1.0   \n",
       "dependents            1000.0     1.155     0.362086    1.0     1.0     1.0   \n",
       "\n",
       "                          75%      max  \n",
       "months_loan_duration    24.00     72.0  \n",
       "amount                3972.25  18424.0  \n",
       "percent_of_income        4.00      4.0  \n",
       "years_at_residence       4.00      4.0  \n",
       "age                     42.00     75.0  \n",
       "existing_loans_count     2.00      4.0  \n",
       "dependents               1.00      2.0  \n",
       "\n",
       "Here are the non-numeric columns and corresponding value counts:\n",
       "\n",
       "`checking_balance`: {'unknown': 394, '< 0 DM': 274, '1 - 200 DM': 269, '> 200 DM': 63}\n",
       "`credit_history`: {'good': 530, 'critical': 293, 'poor': 88, 'very good': 49, 'perfect': 40}\n",
       "`purpose`: {'furniture/appliances': 473, 'car': 337, 'business': 97, 'education': 59, 'renovations': 22, 'car0': 12}\n",
       "`savings_balance`: {'< 100 DM': 603, 'unknown': 183, '100 - 500 DM': 103, '500 - 1000 DM': 63, '> 1000 DM': 48}\n",
       "`employment_duration`: {'1 - 4 years': 339, '> 7 years': 253, '4 - 7 years': 174, '< 1 year': 172, 'unemployed': 62}\n",
       "`other_credit`: {'none': 814, 'bank': 139, 'store': 47}\n",
       "`housing`: {'own': 713, 'rent': 179, 'other': 108}\n",
       "`job`: {'skilled': 630, 'unskilled': 200, 'management': 148, 'unemployed': 22}\n",
       "`phone`: {'no': 596, 'yes': 404}\n",
       "`default`: {'no': 700, 'yes': 300}\n",
       "\n",
       "\n",
       "Use both the numeric and non-numeric columns as appropriate.\n",
       "```\n",
       "\n",
       "----\n",
       "\n",
       "Here is the question:\n",
       "\n",
       "```\n",
       "Using `my_df`, group by checking account and calculate the average duration of the loan. Print the resulting dataframe.\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mprint(workflow.history()[0].prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Cost:           $0.00104\n",
      "Total Tokens:          946\n",
      "Total Prompt Tokens:   855\n",
      "Total Response Tokens: 91\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total Cost:           ${model.cost:.5f}\")\n",
    "print(f\"Total Tokens:          {model.total_tokens:,}\")\n",
    "print(f\"Total Prompt Tokens:   {model.input_tokens:,}\")\n",
    "print(f\"Total Response Tokens: {model.response_tokens:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You're welcome! If you have any more questions or need further assistance, feel free to ask.\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow(\"thank you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'thank you'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# no metadata used even though we are using same prompt\n",
    "workflow.history()[-1].prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Cost:           $0.00203\n",
      "Total Tokens:          1,922\n",
      "Total Prompt Tokens:   1,811\n",
      "Total Response Tokens: 111\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total Cost:           ${model.cost:.5f}\")\n",
    "print(f\"Total Tokens:          {model.total_tokens:,}\")\n",
    "print(f\"Total Prompt Tokens:   {model.input_tokens:,}\")\n",
    "print(f\"Total Response Tokens: {model.response_tokens:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
